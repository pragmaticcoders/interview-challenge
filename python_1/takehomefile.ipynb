{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b52832",
   "metadata": {},
   "source": [
    "# Data Engineer Take Home Exercise\n",
    "\n",
    "## Question\n",
    "### Data Prep\n",
    "\n",
    "Write a script to transform input CSV to desired output CSV and Parquet. \n",
    "\n",
    "You will find a CSV file in the files folder under `data.csv`. There are three steps to this part of the test. Each step concerns manipulating the values for a single field according to the step's requirements. The steps are as follows:\n",
    "\n",
    "**String cleaning** - The bio field contains text with arbitrary padding, spacing and line breaks. Normalize these values to a space-delimited string.\n",
    "\n",
    "**Code swap** - There is a supplementary CSV in the files folder under `state_abbreviations`. This \"data dictionary\" contains state abbreviations alongside state names. For the state field of the input CSV, replace each state abbreviation with its associated state name from the data dictionary.\n",
    "\n",
    "**Date offset** - The start_date field contains data in a variety of formats. These may include e.g., \"June 23, 1912\" or \"5/11/1930\" (month, day, year). But not all values are valid dates. Invalid dates may include e.g., \"June 2018\", \"3/06\" (incomplete dates) or even arbitrary natural language. Add a start_date_description field adjacent to the start_date column to filter invalid date values into. Normalize all valid date values in start_date to ISO 8601 (i.e., YYYY-MM-DD).\n",
    "\n",
    "Your script should take `data.csv` as input and produce a cleansed `enriched.csv` and `enriched.snappy.parquet` files according to the step requirements above.\n",
    "\n",
    "## Submission Guidelines\n",
    "We ask that your solutions be implemented in Python (3.8 or newer) or PySpark (3.3 or newer). If you would like to present skills for both approach, feel free to prepare two separate jupyter notebooks. Assume that code will be used monthly to process the data and store it in AWS S3 based data lake. With that assumption please prepare for discussion how this code can be scheduled and how outputs should be stored in S3 bucket.\n",
    "\n",
    "### Assessment Criteria\n",
    "Our goal is not to fool you. On the contrary, we would like to see you in your best light! We value clean, DRY and documented code; and in the interest of full disclosure, our assessment criteria is outlined below (in order of significance):\n",
    "\n",
    "1. Your ability to effectively solve the problems posed.\n",
    "1. Your ability to solve these problems in a clear and logical manner, with tasteful design.\n",
    "1. Your ability to appropriately document and comment your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9972ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
